{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5ZnvkYE+VLFClVmfNflLO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tgoutam/skills-introduction-to-github/blob/main/Hugging%20Face%20Sentiment%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogah3s33Qhio"
      },
      "outputs": [],
      "source": [
        "# prompt: install transformer and pyTorch library\n",
        "\n",
        "!pip install transformers\n",
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
      ],
      "metadata": {
        "id": "X054i8MBRBPe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Import the torch library"
      ],
      "metadata": {
        "id": "-d4LcP7VbiTe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "KF4Q_iEuSD4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenize the Input Text** - Prepare your input text by tokenizing it. Tokenization converts the text into a format that the model can process."
      ],
      "metadata": {
        "id": "yWCpzPB6Zbjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"I HATE using Hugging Face transformers!\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "3ETHXgX2Y9It"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the Model**\n",
        "Pass the tokenized input to the model to get predictions.\n",
        "\n",
        "Explanation\n",
        "with torch.no_grad(): This context manager is used to disable gradient calculation, which is not needed for inference and reduces memory usage.\n",
        "\n",
        "outputs = model(**inputs): This passes the tokenized inputs to the model and stores the output. The **inputs syntax unpacks the dictionary of inputs so that each key-value pair is passed as a separate argument.\n",
        "\n"
      ],
      "metadata": {
        "id": "3f03JLy6Zg-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n"
      ],
      "metadata": {
        "id": "632arrUbaCLH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process the Output**\n",
        "Extract and process the output to interpret the results."
      ],
      "metadata": {
        "id": "GZWgCSXkcRUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = outputs.logits.argmax(dim=-1)\n"
      ],
      "metadata": {
        "id": "UM5Yipk3cS5k"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This extracts the logits (raw model outputs) from the output, and argmax(dim=-1) finds the index of the highest logit, which corresponds to the predicted class.**"
      ],
      "metadata": {
        "id": "GOB0kDqOdFV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Predicted class: {predictions.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_ZGraDUdQjU",
        "outputId": "5fe1aca6-4b7e-4951-8014-98b047c9d464"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicted class: 0 means the Sentiment expressed above in (\" I HATE...\") is NEGATIVE. If HATE is changed to LOVE and re-run, the Sentiment shows as 1 i.e POSITIVE."
      ],
      "metadata": {
        "id": "M7ddIScueR60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Python Code:\n",
        "\n",
        "# Step 1: Install necessary libraries\n",
        "!pip install transformers torch\n",
        "\n",
        "# Step 2: Import libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Step 3: Select and load a model\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Step 4: Tokenize the input text\n",
        "input_text = \"I love using Hugging Face transformers!\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Step 5: Run the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Step 6: Process the output\n",
        "predictions = outputs.logits.argmax(dim=-1)\n",
        "print(f\"Predicted class: {predictions.item()}\")\n",
        "\n",
        "-------------------\n",
        "\n",
        "To run a Transformer model from the Hugging Face Hub in Google Colab, follow these steps:\n",
        "\n",
        "### Step 1: Install Necessary Libraries\n",
        "First, ensure you have the necessary libraries installed. You'll need the `transformers` library from Hugging Face and `torch` for PyTorch.\n",
        "\n",
        "```python\n",
        "!pip install transformers torch\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "- `!pip install transformers torch`: This command installs the `transformers` library, which provides the tools to work with transformer models, and `torch`, which is PyTorch, a deep learning framework required by most Hugging Face models.\n",
        "\n",
        "### Step 2: Import Libraries\n",
        "Next, import the necessary modules from the `transformers` library.\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "- `from transformers import AutoTokenizer, AutoModelForSequenceClassification`: This imports the `AutoTokenizer` and `AutoModelForSequenceClassification` classes, which are used to load the tokenizer and model, respectively. The `AutoModelForSequenceClassification` class is specifically for classification tasks; if you need a different type of model, you might import a different class.\n",
        "\n",
        "### Step 3: Select and Load a Model\n",
        "Choose a model from the Hugging Face Hub. For this example, we'll use `distilbert-base-uncased-finetuned-sst-2-english`, a sentiment analysis model.\n",
        "\n",
        "```python\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "- `model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"`: This specifies the name of the model you want to use.\n",
        "- `tokenizer = AutoTokenizer.from_pretrained(model_name)`: This loads the tokenizer associated with the specified model, which will preprocess the text data into the format expected by the model.\n",
        "- `model = AutoModelForSequenceClassification.from_pretrained(model_name)`: This loads the pre-trained model itself.\n",
        "\n",
        "### Step 4: Tokenize the Input Text\n",
        "Prepare your input text by tokenizing it. Tokenization converts the text into a format that the model can process.\n",
        "\n",
        "```python\n",
        "input_text = \"I love using Hugging Face transformers!\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "- `input_text = \"I love using Hugging Face transformers!\"`: This is the input text you want to classify.\n",
        "- `inputs = tokenizer(input_text, return_tensors=\"pt\")`: This tokenizes the input text and returns it as PyTorch tensors (`\"pt\"` stands for PyTorch).\n",
        "\n",
        "### Step 5: Run the Model\n",
        "Pass the tokenized input to the model to get predictions.\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "- `with torch.no_grad()`: This context manager is used to disable gradient calculation, which is not needed for inference and reduces memory usage.\n",
        "- `outputs = model(**inputs)`: This passes the tokenized inputs to the model and stores the output. The `**inputs` syntax unpacks the dictionary of inputs so that each key-value pair is passed as a separate argument.\n",
        "\n",
        "### Step 6: Process the Output\n",
        "Extract and process the output to interpret the results.\n",
        "\n",
        "```python\n",
        "predictions = outputs.logits.argmax(dim=-1)\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "- `predictions = outputs.logits.argmax(dim=-1)`: This extracts the logits (raw model outputs) from the output, and `argmax(dim=-1)` finds the index of the highest logit, which corresponds to the predicted class.\n",
        "\n",
        "### Complete Example Code\n",
        "Here is the complete code with all the steps combined:\n",
        "\n",
        "```python\n",
        "# Step 1: Install necessary libraries\n",
        "!pip install transformers torch\n",
        "\n",
        "# Step 2: Import libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Step 3: Select and load a model\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Step 4: Tokenize the input text\n",
        "input_text = \"I love using Hugging Face transformers!\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Step 5: Run the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Step 6: Process the output\n",
        "predictions = outputs.logits.argmax(dim=-1)\n",
        "print(f\"Predicted class: {predictions.item()}\")\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "- `print(f\"Predicted class: {predictions.item()}\")`:\n",
        "\n",
        "This prints the predicted class label. In sentiment analysis, 0 typically represents negative sentiment and 1 represents positive sentiment.\n",
        "\n",
        "This example demonstrates how to load, prepare, and run a Transformer model from the Hugging Face Hub in Google Colab, and how to interpret the results.\n"
      ],
      "metadata": {
        "id": "FNjSDJR9fTNw"
      }
    }
  ]
}